?file.path
?file.exists
?row
row.names
?row.names
?data.frame
?tail
?tools::checkFF
tools::checkFF("PytrendsLongitudnalR")
?tools::checkFF
sessionInfo()
rcmdcheck::do_check()
library(rcmdcheck)
rcmdcheck::do_check()
tools::summarize_CRAN_check_status()
tools::summarize_CRAN_check_status("PytrendsLongitudnalR")
?tools::summarize_CRAN_check_status()
devtools::build_vignettes("PytrendsLongitudnalR")
devtools::build_vignettes()
devtools::document()
devtools::build()
devtools::build_vignettes()
devtools::document()
devtools::build()
n
devtools::document()
devtools::document()
devtools::build()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::build()
devtools::document()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::build()
devtools::install()
library(PytrendsLongitudnalR)
params <- initialize_request_trends(
keyword = "Joe Biden",
topic = "/m/012gx2",
folder_name = "biden_pack",
start_date = "2017-12-03",  # Date string
end_date = "2024-05-19",     # Date string
data_format = "weekly"
)
time_series(params, reference_geo_code = "US-CA")
cross_section(params, geo = "US", resolution="REGION")
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
tempdir()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
setwd("/Users/malika/Desktop/Rtrends-longitudnal")
library(lubridate)
library(dplyr)
library(jsonlite)
library(log4r)
library(glue)
library(tibble)
library(readr)
library(R6)
library(reticulate)
# Import Python libraries
pd <- import("pandas")
os <- import("os")
glob <- import("glob")
json <- import("json")
requests <- import("requests")
ResponseError <- import("pytrends.exceptions", convert = FALSE)$ResponseError
TrendReq <- import("pytrends.request", convert = FALSE)$TrendReq
dt <- import("datetime")
relativedelta <- import("dateutil.relativedelta")
time <- import("time")
logging <- import("logging")
console <- import("rich.console", convert = FALSE)$Console
RichHandler <- import("rich.logging", convert = FALSE)$RichHandler
math <- import("math")
platform <- import("platform")
logging$basicConfig(
level = "INFO",
format = "%(message)s",
datefmt = "%d-%b-%Y %H:%M:%S",
handlers = list(RichHandler(rich_tracebacks = TRUE))
)
initialize_request_trends <- function(keyword, topic, folder_name, start_date, end_date, data_format) {
# Convert date strings to R Date objects
start_date <- as.Date(start_date)
end_date <- as.Date(end_date)
# Calculate number of days as integer
num_of_days <- as.integer(end_date - start_date)
# Initialize logger
logger <- logging$getLogger("rich")
# Validate data_format
if (!data_format %in% c('daily', 'weekly', 'monthly')) {
stop("data_format should be 'daily'/'weekly'/'monthly'")
} else if (data_format == 'monthly') {
start_date <- as.Date(paste0(format(start_date, "%Y-%m"), "-01"))
if (num_of_days < 1890) {
stop(sprintf("Difference Between Start and End date needs to be more than 1889 days to get monthly data. Given only '%d' days", num_of_days))
}
} else if (data_format == 'weekly') {
if (num_of_days < 270) {
stop(sprintf("Difference Between Start and End date needs to be more than 269 days to get weekly data. Given '%d' days", num_of_days))
}
if (num_of_days < 270) {
stop(sprintf("Difference Between Start and End date needs to be more than 269 days to get weekly data. Given '%d' days", num_of_days))
}
time_window <- 1889
times <- determine_time_periods(start_date, end_date, num_of_days, time_window)
check_end_period <- as.integer(difftime(times[length(times)], times[length(times)-1], units = "days"))
if (check_end_period < 270) {
stop(sprintf("Last Time Period is less than 270 days. Given %d", check_end_period))
}
} else {
time_window <- 269
times <- determine_time_periods(start_date, end_date, num_of_days, time_window)
}
# Create required directories
dir.create(folder_name, showWarnings = FALSE)  # Main folder
dir.create(file.path(folder_name, data_format), showWarnings = FALSE)  # Subfolder
# Save parameters into a JSON file
params <- list(
keyword = keyword,
topic = topic,
folder_name = folder_name,
start_date = as.character(start_date),
end_date = as.character(end_date),
data_format = data_format
)
params_fl <- file.path(folder_name, data_format, "params.txt")
jsonlite::write_json(params, params_fl, pretty = TRUE)
# Initialize pytrends request (example)
pytrend <- TrendReq()
# Return a list of initialized values
list(
logger = logger,
keyword = keyword,
topic = topic,
folder_name = folder_name,
start_date = start_date,
end_date = end_date,
data_format = data_format,
num_of_days = num_of_days,
pytrend = pytrend,
time_window = if (data_format == "monthly") NULL else time_window,
times = if (data_format != "monthly") times else NULL
)
}
determine_time_periods <- function(start_date, end_date, num_of_days, time_window) {
timeperiod <- ceiling(num_of_days / time_window)
times <- seq.Date(start_date, by = paste0(time_window, " days"), length.out = timeperiod + 1)
times[length(times)] <- end_date
return(times)
}
create_required_directory <- function(folder) {
if (!dir.exists(folder)) {
dir.create(folder, recursive = TRUE)
}
}
cross_section <- function(params, geo = "", resolution = "COUNTRY") {
logger <- params$logger
data_format <- params$data_format
folder_name <- params$folder_name
start_date <- params$start_date
end_date <- params$end_date
keyword <- params$keyword
topic <- params$topic
num_of_days <- params$num_of_days
pytrend <- params$pytrend
logger$info("Collecting Cross Section Data now")
res <- c("COUNTRY", "REGION", "CITY")
if (geo == "") {
stop("geo cannot be empty. For worldwide data, geo='Worldwide'")
}
if ((geo == "" && resolution != "COUNTRY") || (geo != "" && !(resolution %in% res))) {
logger$info("Incorrect Resolution Provided. Defaulting to 'COUNTRY'")
resolution <- "COUNTRY"
}
create_required_directory(file.path(folder_name, data_format, "by_region"))
if (data_format == 'daily') {
chng_delta <- lubridate::days(1)
end_delta <- lubridate::days(0)
form <- 'day'
} else if (data_format == 'weekly') {
chng_delta <- lubridate::weeks(1)
end_delta <- lubridate::weeks(1) - lubridate::days(1)
form <- 'week'
} else if (data_format == 'monthly') {
chng_delta <- lubridate::months(1)
end_delta <- lubridate::months(1) - lubridate::days(1)
form <- 'month'
}
current_time <- start_date
i <- 0
logger$info("Please note that this method may take hours to finish. Have patience.", extra = list(markup = TRUE))
while (TRUE) {
if ((data_format == 'weekly' && current_time >= end_date) || (data_format != 'weekly' && current_time > end_date)) {
break
}
current_end_time <- current_time + end_delta
current_time_str <- format(current_time, "%Y-%m-%d")
current_end_time_str <- format(current_end_time, "%Y-%m-%d")
cat(current_time_str,"\n")
cat(current_end_time_str,"\n")
file_path <- file.path(folder_name, data_format, "by_region", sprintf("%s_%d-%s-%s.csv", form, i + 1, current_time_str, current_end_time_str))
if (file.exists(file_path)) {
logger$info(sprintf("Data for %s-%s already collected. Moving to next date...", current_time_str, current_end_time_str), extra = list(markup = TRUE))
current_time <- current_time + chng_delta
i <- i + 1
} else {
tryCatch({
print(pytrend)
print(topic)
print(geo)
print(sprintf('%s %s', current_time_str, current_end_time_str))
pytrend$build_payload(kw_list = list(topic), geo = geo, timeframe = sprintf('%s %s', current_time_str, current_end_time_str))
#pytrend$build_payload(kw_list = list("/m/012gx2"), geo = geo, timeframe= sprintf('%s %s', current_time_str, current_end_time_str))
print(pytrend)
Sys.sleep(5)
df <- pytrend$interest_by_region(resolution = resolution, inc_geo_code = TRUE, inc_low_vol = TRUE)
print(pytrend)
#df$rename(dict(topic = keyword), inplace = TRUE)
df <- reticulate::py_to_r(df)
print(head(df))
names(df)[names(df) == keyword] <- keyword
i <- i + 1
#utils::write.csv(df, file_path, row.names = FALSE)
write.csv(df, file = file.path(folder_name, data_format, "by_region", paste0(form, "_", i, "-", format(current_time, "%Y%m%d"), "-", format(current_end_time, "%Y%m%d"), ".csv")))
}, error = function(e) {
if (inherits(e, "ResponseError")) {
logger$info("Please have patience as we reset rate limit ... ", extra = list(markup = TRUE))
Sys.sleep(5)
} else {
stop(e)
}
})
Sys.sleep(5)
current_time <- current_time + chng_delta
}
}
logger$info("[bold green]Successfully Collected Cross Section Data![/]", extra = list(markup = TRUE))
}
# Time Series Data collection method for 'monthly'
time_series_monthly <- function(params, reference_geo_code = "US") {
logger <- params$logger
pytrend <- params$pytrend
folder_name <- params$folder_name
data_format <- params$data_format
start_date <- params$start_date
end_date <- params$end_date
keyword <- params$keyword
topic <- params$topic
# Create file path
file_path <- file.path(folder_name, data_format, "over_time", reference_geo_code, sprintf("%s-%s.csv", format(start_date, "%Y%m%d"), format(end_date, "%Y%m%d")))
# Check if file already exists
if (file.exists(file_path)) {
logger$info("All Data for current request is already collected", extra = list(markup = TRUE))
} else {
tryCatch({
# Build the payload
pytrend$build_payload(kw_list = list(topic), geo = reference_geo_code,
timeframe = sprintf('%s %s', format(start_date, "%Y-%m-%d"), format(end_date, "%Y-%m-%d")))
Sys.sleep(5)
# Get interest over time data
df <- pytrend$interest_over_time()
# Convert to R dataframe and rename columns
df <- reticulate::py_to_r(df)
#colnames(df_r)[colnames(df_r) == topic] <- keyword
# Save the dataframe to a CSV file
write.csv(df, file_path)
}, ResponseError = function(e) {
logger$info("Please have patience as we reset rate limit ... ", extra = list(markup = TRUE))
Sys.sleep(5)
}, error = function(e) {
logger$error(sprintf("[bold red]Whoops![/] An error occurred during the request for period %d: %s", period, e$message), exc_info = TRUE, extra = list(markup = TRUE))
})
}
}
# Time Series Data collection method for 'weekly'/'daily'
time_series_nmonthly <- function(params, reference_geo_code = "US") {
logger <- params$logger
pytrend <- params$pytrend
folder_name <- params$folder_name
data_format <- params$data_format
times <- params$times
keyword <- params$keyword
topic <- params$topic
for (period in 1:(length(times) - 1)) {
start <- times[[period]]
end <- times[[period + 1]]
#print(start, "\n")
#print(end, "\n")
if (data_format == "weekly") {
num_days <- as.integer(difftime(end, start, units = "days"))
if (num_days < 270) {
#stats$stop()
stop(sprintf("For period: %d, days given: %d. Please increase timeline", period, num_days))
}
}
file_path <- file.path(folder_name, data_format, "over_time", reference_geo_code,
sprintf("%d-%s-%s.csv", period, format(start, "%Y%m%d"), format(end, "%Y%m%d")))
if (file.exists(file_path)) {
logger$info(sprintf("Data for %s to %s already collected. Moving to next date...", format(start, "%d/%m/%Y"), format(end, "%d/%m/%Y")), extra = list(markup = TRUE))
} else {
tryCatch({
print(pytrend, "\n")
#print(sprintf('%s %s', format(start, "%Y-%m-%d"), format(end, "%Y-%m-%d")))
pytrend$build_payload(kw_list = list(topic), geo = reference_geo_code,
timeframe = sprintf('%s %s', format(start, "%Y-%m-%d"), format(end, "%Y-%m-%d")))
Sys.sleep(5)
print(pytrend, "\n")
df <- pytrend$interest_over_time()
df <- reticulate::py_to_r(df)
if (nrow(df) == 0) {
logger$info(sprintf("No Data was returned for period: %d -> '%s' to '%s'", period, format(start, "%d/%m/%Y"), format(end, "%d/%m/%Y")))
} else {
#print("df has non 0 rows")
names(df)[names(df) == topic] <- keyword
if ("isPartial" %in% names(df)) {
df <- df[, !names(df) %in% "isPartial", drop = FALSE]
}
print(head(df))
write.csv(df, file_path)
}
}, ResponseError = function(e) {
logger$info("Please have patience as we reset rate limit ... ", extra = list(markup = TRUE))
Sys.sleep(5)
}, error = function(e) {
#stats$stop()
logger$error(sprintf("[bold red]Whoops![/] An error occurred during the request for period %d: %s", period, e$message), exc_info = TRUE, extra = list(markup = TRUE))
})
}
}
}
time_series <- function(params, reference_geo_code = "US") {
logger <- params$logger
folder_name <- params$folder_name
data_format <- params$data_format
time_window <- params$time_window
#print(params)
print(params$time_window)
logger$info("Collecting Over Time Data now")
create_required_directory(file.path(folder_name, data_format, "over_time"))
create_required_directory(file.path(folder_name, data_format, "over_time", reference_geo_code))
if (!is.null(time_window)) {
time_series_nmonthly(params, reference_geo_code)
} else {
print("minthly")
time_series_monthly(params, reference_geo_code)
}
logger$info("[bold green]Collected Time Series Data![/]")
}
concat_time_series <- function(params, reference_geo_code = "US", zero_replace = 0.1) {
logger <- params$logger
folder_name <- params$folder_name
data_format <- params$data_format
keyword <- params$keyword
print(keyword)
logger$info("Concatenating Over Time data now", extra = list(markup = TRUE))
# Create Folder to save the concatenated time series data
create_required_directory(file.path(folder_name, data_format, "concat_time_series"))
path_to_time_data <- file.path(folder_name, data_format, "over_time", reference_geo_code)
# List to store DataFrames
dfs <- list()
# Read each CSV file into a DataFrame and store in dfs list
files <- list.files(path_to_time_data, full.names = TRUE)
for (file in files) {
df <- read.csv(file, check.names = FALSE)
dfs[[length(dfs) + 1]] <- df
}
df <- df[, colSums(is.na(df)) != nrow(df)]
# Replace zeros with zero_replace value
for (i in seq_along(dfs)) {
dfs[[i]][dfs[[i]][[keyword]] == 0, keyword] <- zero_replace
}
# Concatenate the time series data
prev_window <- dfs[[1]]
#print(prev_window)
for (periods in 2:length(dfs)) {
#print(periods)
next_window <- dfs[[periods]]
prev_window_multiplier <- 100 / prev_window[nrow(prev_window), keyword]
next_window_multiplier <- 100 / next_window[1, keyword]
prev_window[, keyword] <- prev_window[, keyword] * prev_window_multiplier
next_window[, keyword] <- next_window[, keyword] * next_window_multiplier
prev_window <- rbind(prev_window[-nrow(prev_window), ], next_window)
#print(prev_window[, keyword])
}
# Write concatenated DataFrame to CSV
concat_file_path <- file.path(folder_name, data_format, "concat_time_series", paste0(reference_geo_code, ".csv"))
write.csv(prev_window, concat_file_path, row.names = FALSE)
logger$info("[bold green]Concatenation Complete! :)[/]", extra = list(markup = TRUE))
}
convert_cross_section <- function(params, reference_geo_code = "US", zero_replace = 0.1) {
logger <- params$logger
folder_name <- params$folder_name
data_format <- params$data_format
keyword <- params$keyword
start_date <- params$start_date
end_date <- params$end_date
logger$info("Rescaling cross section Data now", extra = list(markup = TRUE))
# Create required directories
create_required_directory(file.path(folder_name, data_format, "converted"))
create_required_directory(file.path(folder_name, data_format, "converted", reference_geo_code))
concat_file_path <- file.path(folder_name, data_format, "concat_time_series", paste0(reference_geo_code, ".csv"))
if (file.exists(concat_file_path)) {
time_series_concat <- read.csv(concat_file_path, header = TRUE, check.names = FALSE)
} else {
files_in_over_time <- list.files(file.path(folder_name, data_format, "over_time", reference_geo_code), full.names = TRUE)
time_series_concat <- read.csv(files_in_over_time, header = TRUE, check.names = FALSE)
#print(as.numeric(time_series_concat[[keyword]][10]))
}
names(time_series_concat)[names(time_series_concat) == ''] <- 'date'
print(colnames(time_series_concat))
# Initialize empty data frame for conversion result
conv <- data.frame()
# Iterate over rows in time_series_concat
for (ind in seq_len(nrow(time_series_concat))) {
record <- format(as.POSIXct(as.character(time_series_concat$date[ind]), format = "%Y-%m-%d %H:%M:%S"), "%Y%m%d")
time_ind <- as.numeric(time_series_concat[[keyword]][ind])
#cat(record, "\n")
#cat(time_ind, "\n")
# Construct snapshot file path based on record date
snap_file <- list.files(path = file.path(folder_name, data_format, "by_region"), pattern = paste0(".*", record, ".*csv"), full.names = TRUE)[1]
#print(snap_file)
# Extract column name from snapshot file
if (Sys.info()['sysname'] == "win32") {
fl_name <- tail(unlist(strsplit(snap_file, "\\\\")), 1)
} else {
fl_name <- tail(unlist(strsplit(snap_file, "/")), 1)
#print(fl_name)
}
col_name <- gsub("\\.csv", "", fl_name)
#cat(col_name)
# Read snapshot data
snap_df <- read.csv(snap_file, header = TRUE, stringsAsFactors = FALSE, na.strings = "", check.names = FALSE)
#print(head(snap_df))
snap_df[[keyword]][is.na(snap_df[[keyword]])] <- zero_replace
# Find reference value based on geoCode
ref_value <- as.numeric(snap_df[snap_df$geoCode == reference_geo_code, keyword])
#print(ref_value)
# Calculate conversion multiplier
conv_multiplier <- time_ind / ref_value
#print(conv_multiplier)
# Perform conversion on snapshot dataframe
snap_df[[col_name]] <- round(snap_df[[keyword]] * conv_multiplier, 2)
print(snap_df[[col_name]])
#print(conv)
# Collect initial geoName and geoCode if it's the first iteration
if (ind == 1) {
conv <- snap_df[c("geoName", "geoCode")]
#print(ref_value)
#print(conv_multiplier)
}
# Append converted data to conv data frame
#conv[[col_name]] <- snap_df[[keyword]]
conv[[col_name]] <- snap_df[[col_name]]
#print(snap_df[[keyword]])
}
# Write converted DataFrame to CSV
write.csv(conv, file.path(folder_name, data_format, "converted", reference_geo_code, paste0("final-converted-",
format(start_date, "%Y%m%d"), "-",
format(end_date, "%Y%m%d"), ".csv")), row.names = FALSE)
logger$info("[bold green]DONE Converting! :) [/]", extra = list(markup = TRUE))
}
# Example usage
params <- initialize_request_trends(
keyword = "Joe Biden",
topic = "/m/012gx2",
folder_name = "bidennn",
start_date = "2019-12-29",  # Date string
end_date = "2024-05-19",     # Date string
data_format = "weekly"
)
cross_section(params, geo = "US", resolution="REGION")
getwd()
